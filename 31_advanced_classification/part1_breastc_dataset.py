import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ========================================
# PARTEA 1: ÃNCÄ‚RCAREA DATASET-ULUI
# ========================================

# ÃncarcÄƒ dataset-ul breast cancer de la sklearn
cancer_data = load_breast_cancer()

print("ğŸ“Š INFORMAÈšII DESPRE DATASET:")
print(f"NumÄƒr de sample: {cancer_data.data.shape[0]}")
print(f"NumÄƒr de features: {cancer_data.data.shape[1]}")
print(f"Clase: {cancer_data.target_names}")
print()

# CreeazÄƒ DataFrame pentru o vizualizare mai bunÄƒ
df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)
df['target'] = cancer_data.target

print("ğŸ” PRIMELE 5 RÃ‚NDURI:")
print(df.head())
print()

# ========================================
# PARTEA 2: EXPLORAREA DATELOR
# ========================================

print("ğŸ“ˆ STATISTICI DESCRIPTIVE:")
print(df.describe())
print()

# VerificÄƒ distribuÈ›ia claselor
print("âš–ï¸ DISTRIBUÈšIA CLASELOR:")
print(f"MalignÄƒ (0): {sum(cancer_data.target == 0)} paciente")
print(f"BenignÄƒ (1): {sum(cancer_data.target == 1)} paciente")
print()

# Vizualizare: DistribuÈ›ia primelor 4 features
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
features_to_plot = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']

for idx, feature in enumerate(features_to_plot):
    ax = axes[idx // 2, idx % 2]

    # HistogramÄƒ pentru fiecare clasÄƒ
    df[df['target'] == 0][feature].hist(ax=ax, alpha=0.5, label='MalignÄƒ',
                                         color='red', bins=30)
    df[df['target'] == 1][feature].hist(ax=ax, alpha=0.5, label='BenignÄƒ',
                                         color='green', bins=30)

    ax.set_xlabel(feature)
    ax.set_ylabel('FrecvenÈ›Äƒ')
    ax.set_title(f'DistribuÈ›ia: {feature}')
    ax.legend()

plt.tight_layout()
plt.savefig('breast_cancer_features_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… Grafic salvat: breast_cancer_features_distribution.png")
print()

# ========================================
# PARTEA 3: PREGÄ‚TIREA DATELOR
# ========================================

# Separare features (X) È™i target (y)
X = cancer_data.data
y = cancer_data.target

# Split Ã®n train È™i test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("ğŸ“¦ SPLIT TRAIN-TEST:")
print(f"Training set: {X_train.shape[0]} sample")
print(f"Test set: {X_test.shape[0]} sample")
print()

# ========================================
# PARTEA 4: NORMALIZARE (CRUCIAL!)
# ========================================

# IMPORTANT: fit_transform() pe train, doar transform() pe test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("ğŸ”§ NORMALIZARE COMPLETÄ‚:")
print(f"Ãnainte - Mean prima feature train: {X_train[:, 0].mean():.2f}")
print(f"DupÄƒ - Mean prima feature train: {X_train_scaled[:, 0].mean():.2f}")
print(f"DupÄƒ - Std prima feature train: {X_train_scaled[:, 0].std():.2f}")
print()

# Vizualizare: Efect normalizare
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Ãnainte de normalizare
ax1.boxplot([X_train[:, i] for i in range(5)], tick_labels=cancer_data.feature_names[:5])
ax1.set_title('Ãnainte de Normalizare', fontsize=14, fontweight='bold')
ax1.set_ylabel('Valoare')
ax1.tick_params(axis='x', rotation=45)

# DupÄƒ normalizare
ax2.boxplot([X_train_scaled[:, i] for i in range(5)], tick_labels=cancer_data.feature_names[:5])
ax2.set_title('DupÄƒ Normalizare (StandardScaler)', fontsize=14, fontweight='bold')
ax2.set_ylabel('Valoare NormalizatÄƒ')
ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('normalization_effect.png', dpi=300, bbox_inches='tight')
print("âœ… Grafic salvat: normalization_effect.png")

# ========================================
# DE CE NORMALIZARE?
# ========================================
print("""
ğŸ¯ DE CE ESTE NORMALIZAREA CRUCIALÄ‚?

1. SCARA DIFERITÄ‚ A FEATURES:
   - 'mean radius': 6-28 (diferenÈ›Äƒ de ~22)
   - 'mean area': 143-2501 (diferenÈ›Äƒ de ~2358)

   FÄƒrÄƒ normalizare, 'mean area' ar domina modelul!

2. ALGORITMI SENSIBILI:
   - SVM: bazat pe distanÈ›e â†’ trebuie scale similar
   - KNN: distanÈ›a EuclideanÄƒ â†’ trebuie scale similar
   - Neural Networks: converge mai repede cu date normalizate

3. INTERPRETARE:
   - DupÄƒ normalizare: toate features au contribuÈ›ie echitabilÄƒ
   - CoeficienÈ›ii modelului sunt comparabili

ğŸ”’ REGULA DE AUR: fit_transform() DOAR pe TRAIN!
   - Test set-ul NU TREBUIE sÄƒ influenÈ›eze media/std
   - Altfel â†’ DATA LEAKAGE â†’ rezultate false
""")