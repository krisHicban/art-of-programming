from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd

# Presupunem cÄƒ avem X_train, X_test, y_train, y_test

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ========================================
# PARTEA 1: ÃNCÄ‚RCAREA DATASET-ULUI
# ========================================

# ÃncarcÄƒ dataset-ul breast cancer de la sklearn
cancer_data = load_breast_cancer()

print("ğŸ“Š INFORMAÈšII DESPRE DATASET:")
print(f"NumÄƒr de sample: {cancer_data.data.shape[0]}")
print(f"NumÄƒr de features: {cancer_data.data.shape[1]}")
print(f"Clase: {cancer_data.target_names}")
print()

# CreeazÄƒ DataFrame pentru o vizualizare mai bunÄƒ
df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)
df['target'] = cancer_data.target

print("ğŸ” PRIMELE 5 RÃ‚NDURI:")
print(df.head())
print()

# ========================================
# PARTEA 2: EXPLORAREA DATELOR
# ========================================

print("ğŸ“ˆ STATISTICI DESCRIPTIVE:")
print(df.describe())
print()

# VerificÄƒ distribuÈ›ia claselor
print("âš–ï¸ DISTRIBUÈšIA CLASELOR:")
print(f"MalignÄƒ (0): {sum(cancer_data.target == 0)} paciente")
print(f"BenignÄƒ (1): {sum(cancer_data.target == 1)} paciente")
print()

# Vizualizare: DistribuÈ›ia primelor 4 features
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
features_to_plot = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']

for idx, feature in enumerate(features_to_plot):
    ax = axes[idx // 2, idx % 2]

    # HistogramÄƒ pentru fiecare clasÄƒ
    df[df['target'] == 0][feature].hist(ax=ax, alpha=0.5, label='MalignÄƒ',
                                         color='red', bins=30)
    df[df['target'] == 1][feature].hist(ax=ax, alpha=0.5, label='BenignÄƒ',
                                         color='green', bins=30)

    ax.set_xlabel(feature)
    ax.set_ylabel('FrecvenÈ›Äƒ')
    ax.set_title(f'DistribuÈ›ia: {feature}')
    ax.legend()

plt.tight_layout()
plt.savefig('breast_cancer_features_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… Grafic salvat: breast_cancer_features_distribution.png")
print()

# ========================================
# PARTEA 3: PREGÄ‚TIREA DATELOR
# ========================================

# Separare features (X) È™i target (y)
X = cancer_data.data
y = cancer_data.target

# Split Ã®n train È™i test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("ğŸ“¦ SPLIT TRAIN-TEST:")
print(f"Training set: {X_train.shape[0]} sample")
print(f"Test set: {X_test.shape[0]} sample")
print()

# ========================================
# PARTEA 4: NORMALIZARE (CRUCIAL!)
# ========================================

# IMPORTANT: fit_transform() pe train, doar transform() pe test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)




# ========================================
# GRIDSEARCHCV PENTRU SVM
# ========================================

print("=" * 60)
print("ğŸ”µ GRIDSEARCHCV PENTRU SVM")
print("=" * 60)

# CreeazÄƒ pipeline
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC(random_state=42))
])

# DefineÈ™te grila de parametri
# NOTÄ‚: Pentru pipeline, folosim 'classifier__parametru'
param_grid_svm = {
    'classifier__C': [0.1, 1, 10, 100],
    'classifier__kernel': ['linear', 'rbf'],
    'classifier__gamma': ['scale', 0.001, 0.01, 0.1]
}

print(f"\nğŸ“Š NumÄƒr total de combinaÈ›ii: {len(param_grid_svm['classifier__C']) * len(param_grid_svm['classifier__kernel']) * len(param_grid_svm['classifier__gamma'])}")
print("\nğŸ” Parametri de testat:")
for param, values in param_grid_svm.items():
    print(f"   {param}: {values}")

# CreeazÄƒ GridSearchCV
grid_search_svm = GridSearchCV(
    estimator=svm_pipeline,
    param_grid=param_grid_svm,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1,  # foloseÈ™te toateæ ¸å¿ƒele CPU
    verbose=2  # afiÈ™eazÄƒ progres
)

print("\nâ³ Antrenare Ã®n curs... (poate dura cÃ¢teva minute)")
grid_search_svm.fit(X_train, y_train)

# Rezultate
print("\n" + "=" * 60)
print("âœ… ANTRENARE COMPLETÄ‚!")
print("=" * 60)

print(f"\nğŸ† CELE MAI BUNE PARAMETRI:")
for param, value in grid_search_svm.best_params_.items():
    print(f"   {param}: {value}")

print(f"\nğŸ“Š Cel mai bun score (CV): {grid_search_svm.best_score_:.4f}")

# TesteazÄƒ pe test set
test_score = grid_search_svm.score(X_test, y_test)
print(f"ğŸ“Š Score pe test set: {test_score:.4f}")

# ========================================
# GRIDSEARCHCV PENTRU RANDOM FOREST
# ========================================

print("\n" + "=" * 60)
print("ğŸŸ¢ GRIDSEARCHCV PENTRU RANDOM FOREST")
print("=" * 60)

rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_grid_rf = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [5, 10, 20, None],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

print(f"\nğŸ“Š NumÄƒr total de combinaÈ›ii: {len(param_grid_rf['classifier__n_estimators']) * len(param_grid_rf['classifier__max_depth']) * len(param_grid_rf['classifier__min_samples_split']) * len(param_grid_rf['classifier__min_samples_leaf'])}")

grid_search_rf = GridSearchCV(
    estimator=rf_pipeline,
    param_grid=param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("\nâ³ Antrenare Random Forest...")
grid_search_rf.fit(X_train, y_train)

print(f"\nğŸ† CELE MAI BUNE PARAMETRI:")
for param, value in grid_search_rf.best_params_.items():
    print(f"   {param}: {value}")

print(f"\nğŸ“Š Cel mai bun score (CV): {grid_search_rf.best_score_:.4f}")
print(f"ğŸ“Š Score pe test set: {grid_search_rf.score(X_test, y_test):.4f}")

# ========================================
# GRIDSEARCHCV PENTRU KNN
# ========================================

print("\n" + "=" * 60)
print("ğŸŸ£ GRIDSEARCHCV PENTRU KNN")
print("=" * 60)

knn_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', KNeighborsClassifier())
])

param_grid_knn = {
    'classifier__n_neighbors': [3, 5, 7, 9, 11, 15],
    'classifier__weights': ['uniform', 'distance'],
    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']
}

print(f"\nğŸ“Š NumÄƒr total de combinaÈ›ii: {len(param_grid_knn['classifier__n_neighbors']) * len(param_grid_knn['classifier__weights']) * len(param_grid_knn['classifier__metric'])}")

grid_search_knn = GridSearchCV(
    estimator=knn_pipeline,
    param_grid=param_grid_knn,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

print("\nâ³ Antrenare KNN...")
grid_search_knn.fit(X_train, y_train)

print(f"\nğŸ† CELE MAI BUNE PARAMETRI:")
for param, value in grid_search_knn.best_params_.items():
    print(f"   {param}: {value}")

print(f"\nğŸ“Š Cel mai bun score (CV): {grid_search_knn.best_score_:.4f}")
print(f"ğŸ“Š Score pe test set: {grid_search_knn.score(X_test, y_test):.4f}")

# ========================================
# COMPARAÈšIE FINALÄ‚
# ========================================

print("\n" + "=" * 60)
print("ğŸ† COMPARAÈšIE FINALÄ‚ - DUPÄ‚ TUNING")
print("=" * 60)

comparison_results = pd.DataFrame({
    'Model': ['SVM (tuned)', 'Random Forest (tuned)', 'KNN (tuned)'],
    'CV Score': [
        grid_search_svm.best_score_,
        grid_search_rf.best_score_,
        grid_search_knn.best_score_
    ],
    'Test Score': [
        grid_search_svm.score(X_test, y_test),
        grid_search_rf.score(X_test, y_test),
        grid_search_knn.score(X_test, y_test)
    ]
}).sort_values('Test Score', ascending=False)

print(comparison_results)
print(f"\nğŸ¥‡ CÃ‚È˜TIGÄ‚TORUL: {comparison_results.iloc[0]['Model']}")
print(f"   CV Score: {comparison_results.iloc[0]['CV Score']:.4f}")
print(f"   Test Score: {comparison_results.iloc[0]['Test Score']:.4f}")

# ========================================
# SALVARE MODEL FINAL
# ========================================

print("\n" + "=" * 60)
print("ğŸ’¾ SALVARE MODEL FINAL")
print("=" * 60)

import joblib

# SalveazÄƒ cel mai bun model (presupunem cÄƒ e SVM)
best_model = grid_search_svm.best_estimator_
joblib.dump(best_model, 'breast_cancer_best_model.pkl')

print("\nâœ… Model salvat: breast_cancer_best_model.pkl")
print(f"\nâ„¹ï¸ Modelul salvat include:")
print("   1. StandardScaler cu parametrii antrenaÈ›i")
print("   2. SVM cu hyperparametri optimizaÈ›i")
print("   3. Gata de deployment Ã®n producÈ›ie!")

# ========================================
# ANALIZÄ‚ DETALIATÄ‚ REZULTATE GRIDSEARCH
# ========================================

print("\n" + "=" * 60)
print("ğŸ“Š ANALIZA DETALIATÄ‚ - TOP 10 COMBINAÈšII SVM")
print("=" * 60)

cv_results = pd.DataFrame(grid_search_svm.cv_results_)
top_10 = cv_results.nsmallest(10, 'rank_test_score')[
    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
]

print(top_10.to_string(index=False))

print("""
\nğŸ’¡ INTERPRETARE:
   - mean_test_score: Media acurateÈ›ei pe 5 folduri
   - std_test_score: DeviaÈ›ia standard (variabilitate)
   - rank_test_score: Ranking (1 = cel mai bun)

   ğŸ¯ CÄƒutÄƒm: score mare + std mic = model stabil È™i performant!
""")





# =============================================================================
# BIG PICTURE (one sentence)
# Trained several models, tried many settings for each, and GridSearchCV
# automatically picked the settings that worked best based on cross-validation accuracy.
# That's it. Everything else is details.
# =============================================================================

# =============================================================================
# SVM - INTUITION
# =============================================================================
# SVM tries to draw a line/curve that best separates: malignant vs benign tumors
#
# kernel = rbf â†’ "We allow curved decision boundaries"
#   - linear â†’ straight line
#   - rbf â†’ flexible, curved boundary
#   - ğŸ“Œ Why it won: the data is not linearly separable
#
# C = 10 â†’ "How strict are we about misclassifying points?"
#   - small C â†’ relaxed, simpler boundary
#   - big C â†’ stricter, fits data more closely
#   - ğŸ“Œ C = 10 = good balance between underfitting and overfitting

# =============================================================================
# RANDOM FOREST PARAMETERS (demystified)
# =============================================================================
# classifier__n_estimators: 50 | classifier__max_depth: 10
# classifier__min_samples_split: 5 | classifier__min_samples_leaf: 2
#
# What Random Forest is: A committee of decision trees voting together ğŸŒ³ğŸŒ³ğŸŒ³
#
# n_estimators = 50 â†’ "How many trees are in the forest"
#   - more trees = more stability, diminishing returns after a point
#   - ğŸ“Œ 50 is efficient and stable
#
# max_depth = 10 â†’ "How deep can each tree grow?"
#   - shallow â†’ underfit | too deep â†’ memorizes noise
#   - ğŸ“Œ Depth 10 = controlled complexity
#
# min_samples_split = 5 â†’ "A node needs at least 5 samples to split"
#   - Prevents silly splits on tiny noise
#
# min_samples_leaf = 2 â†’ "Each leaf must have at least 2 samples"
#   - Stops extreme overfitting

# =============================================================================
# SUMMARY BY LEVEL
# =============================================================================
# Level 1: "We tried many models and settings. The computer tested them fairly and chose the best one."
# Level 2: "GridSearch tested different hyperparameters using cross-validation to avoid overfitting."
# Level 3: "SVM with RBF kernel, C=10 and gamma=0.01 gave the best bias-variance tradeoff."