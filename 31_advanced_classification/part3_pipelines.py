from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

# Presupunem cÄƒ avem X_train, X_test, y_train, y_test (NON-scaled!)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split



# ========================================
# PARTEA 1: ÃŽNCÄ‚RCAREA DATASET-ULUI
# ========================================

# ÃŽncarcÄƒ dataset-ul breast cancer de la sklearn
cancer_data = load_breast_cancer()

print("ðŸ“Š INFORMAÈšII DESPRE DATASET:")
print(f"NumÄƒr de sample: {cancer_data.data.shape[0]}")
print(f"NumÄƒr de features: {cancer_data.data.shape[1]}")
print(f"Clase: {cancer_data.target_names}")
print()

# CreeazÄƒ DataFrame pentru o vizualizare mai bunÄƒ
df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)
df['target'] = cancer_data.target

print("ðŸ” PRIMELE 5 RÃ‚NDURI:")
print(df.head())
print()

# ========================================
# PARTEA 2: EXPLORAREA DATELOR
# ========================================

print("ðŸ“ˆ STATISTICI DESCRIPTIVE:")
print(df.describe())
print()

# VerificÄƒ distribuÈ›ia claselor
print("âš–ï¸ DISTRIBUÈšIA CLASELOR:")
print(f"MalignÄƒ (0): {sum(cancer_data.target == 0)} paciente")
print(f"BenignÄƒ (1): {sum(cancer_data.target == 1)} paciente")
print()

# Vizualizare: DistribuÈ›ia primelor 4 features
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
features_to_plot = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']

for idx, feature in enumerate(features_to_plot):
    ax = axes[idx // 2, idx % 2]

    # HistogramÄƒ pentru fiecare clasÄƒ
    df[df['target'] == 0][feature].hist(ax=ax, alpha=0.5, label='MalignÄƒ',
                                         color='red', bins=30)
    df[df['target'] == 1][feature].hist(ax=ax, alpha=0.5, label='BenignÄƒ',
                                         color='green', bins=30)

    ax.set_xlabel(feature)
    ax.set_ylabel('FrecvenÈ›Äƒ')
    ax.set_title(f'DistribuÈ›ia: {feature}')
    ax.legend()

plt.tight_layout()
plt.savefig('breast_cancer_features_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… Grafic salvat: breast_cancer_features_distribution.png")
print()

# ========================================
# PARTEA 3: PREGÄ‚TIREA DATELOR
# ========================================

# Separare features (X) È™i target (y)
X = cancer_data.data
y = cancer_data.target

# Split Ã®n train È™i test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("ðŸ“¦ SPLIT TRAIN-TEST:")
print(f"Training set: {X_train.shape[0]} sample")
print(f"Test set: {X_test.shape[0]} sample")
print()

# ========================================
# PARTEA 4: NORMALIZARE (CRUCIAL!)
# ========================================

# IMPORTANT: fit_transform() pe train, doar transform() pe test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("ðŸ”§ NORMALIZARE COMPLETÄ‚:")
print(f"ÃŽnainte - Mean prima feature train: {X_train[:, 0].mean():.2f}")
print(f"DupÄƒ - Mean prima feature train: {X_train_scaled[:, 0].mean():.2f}")
print(f"DupÄƒ - Std prima feature train: {X_train_scaled[:, 0].std():.2f}")
print()

# Vizualizare: Efect normalizare
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# ÃŽnainte de normalizare
ax1.boxplot([X_train[:, i] for i in range(5)], tick_labels=cancer_data.feature_names[:5])
ax1.set_title('ÃŽnainte de Normalizare', fontsize=14, fontweight='bold')
ax1.set_ylabel('Valoare')
ax1.tick_params(axis='x', rotation=45)

# DupÄƒ normalizare
ax2.boxplot([X_train_scaled[:, i] for i in range(5)], tick_labels=cancer_data.feature_names[:5])
ax2.set_title('DupÄƒ Normalizare (StandardScaler)', fontsize=14, fontweight='bold')
ax2.set_ylabel('Valoare NormalizatÄƒ')
ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('normalization_effect.png', dpi=300, bbox_inches='tight')
print("âœ… Grafic salvat: normalization_effect.png")

































# ========================================
# PIPELINE 1: SVM CU STANDARD SCALER
# ========================================

print("=" * 60)
print("ðŸ”µ PIPELINE 1: StandardScaler â†’ SVM")
print("=" * 60)

# CreeazÄƒ pipeline-ul
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, probability=True))
])

# AntreneazÄƒ pipeline-ul (scalare + antrenare Ã®ntr-un singur pas!)
svm_pipeline.fit(X_train, y_train)

# PredicÈ›ie (scalare + predicÈ›ie automat!)
svm_pred = svm_pipeline.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_pred)

print(f"\nâœ… AcurateÈ›e SVM Pipeline: {svm_accuracy:.4f}")
print(f"\nðŸ“‹ PaÈ™i Ã®n pipeline: {[name for name, _ in svm_pipeline.steps]}")

# ========================================
# PIPELINE 2: RANDOM FOREST
# ========================================

print("\n" + "=" * 60)
print("ðŸŸ¢ PIPELINE 2: StandardScaler â†’ Random Forest")
print("=" * 60)

rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10,
                                         random_state=42))
])

rf_pipeline.fit(X_train, y_train)
rf_pred = rf_pipeline.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

print(f"\nâœ… AcurateÈ›e Random Forest Pipeline: {rf_accuracy:.4f}")

# ========================================
# PIPELINE 3: KNN
# ========================================

print("\n" + "=" * 60)
print("ðŸŸ£ PIPELINE 3: StandardScaler â†’ KNN")
print("=" * 60)

knn_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', KNeighborsClassifier(n_neighbors=5))
])

knn_pipeline.fit(X_train, y_train)
knn_pred = knn_pipeline.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_pred)

print(f"\nâœ… AcurateÈ›e KNN Pipeline: {knn_accuracy:.4f}")

# ========================================
# BENEFICIILE PIPELINE-URILOR
# ========================================

print("\n" + "=" * 60)
print("ðŸŽ¯ BENEFICIILE PIPELINE-URILOR")
print("=" * 60)

print("""
1. ðŸ”’ ZERO DATA LEAKAGE:
   - Scaler Ã®nvaÈ›Äƒ DOAR din train
   - Test nu influenÈ›eazÄƒ niciodatÄƒ transformÄƒrile

2. ðŸ“ COD MAI CURAT:
   - fit() È™i predict() Ã®ntr-un singur apel
   - Nu mai ai nevoie de variabile separate pentru scaled data

3. ðŸ”„ REPRODUCIBILITATE:
   - ÃŽntregul workflow Ã®ntr-un singur obiect
   - PoÈ›i salva pipeline-ul È™i Ã®l foloseÈ™ti identic mai tÃ¢rziu

4. ðŸš€ DEPLOYMENT MAI UÈ˜OR:
   - Un singur obiect de salvat: pickle.dump(pipeline, file)
   - ÃŽn producÈ›ie: pickle.load() â†’ pipeline.predict()

5. ðŸ› ï¸ COMPATIBIL CU GRIDSERCHCV:
   - PoÈ›i optimiza hyperparametri pentru TOÈšI paÈ™ii
   - Cross-validation corectÄƒ automat
""")

# ========================================
# SALVARE È˜I ÃŽNCÄ‚RCARE PIPELINE
# ========================================

print("\n" + "=" * 60)
print("ðŸ’¾ SALVARE È˜I ÃŽNCÄ‚RCARE PIPELINE")
print("=" * 60)

import joblib

# SalveazÄƒ cel mai bun pipeline
best_pipeline = svm_pipeline
joblib.dump(best_pipeline, 'breast_cancer_classifier_pipeline.pkl')
print("\nâœ… Pipeline salvat: breast_cancer_classifier_pipeline.pkl")

# ÃŽncarcÄƒ pipeline-ul
loaded_pipeline = joblib.load('breast_cancer_classifier_pipeline.pkl')
print("âœ… Pipeline Ã®ncÄƒrcat cu succes!")

# TesteazÄƒ cÄƒ funcÈ›ioneazÄƒ identic
loaded_pred = loaded_pipeline.predict(X_test)
loaded_accuracy = accuracy_score(y_test, loaded_pred)
print(f"\nðŸ” AcurateÈ›e pipeline Ã®ncÄƒrcat: {loaded_accuracy:.4f}")
print(f"âœ… Match cu original: {loaded_accuracy == svm_accuracy}")

# ========================================
# PREDICÈšIE PE DATE NOI (SIMULARE)
# ========================================

print("\n" + "=" * 60)
print("ðŸ¥ PREDICÈšIE PE DATE NOI - SIMULARE SPITAL")
print("=" * 60)

# SimuleazÄƒ un pacient nou (30 features)
new_patient = np.array([cancer_data.data[0]])  # folosim prima sample ca exemplu

print("\nðŸ“‹ Date pacient nou (primele 5 features):")
print(new_patient[0][:5])

# PredicÈ›ie cu pipeline (scalare automatÄƒ!)
prediction = loaded_pipeline.predict(new_patient)
prediction_proba = loaded_pipeline.predict_proba(new_patient)

diagnosis = "MalignÄƒ ðŸ”´" if prediction[0] == 0 else "BenignÄƒ ðŸŸ¢"
confidence = max(prediction_proba[0]) * 100

print(f"\nðŸ¥ DIAGNOSTIC: {diagnosis}")
print(f"ðŸ“Š Confidence: {confidence:.2f}%")
print(f"\nðŸ“ˆ ProbabilitÄƒÈ›i:")
print(f"   - MalignÄƒ: {prediction_proba[0][0] * 100:.2f}%")
print(f"   - BenignÄƒ: {prediction_proba[0][1] * 100:.2f}%")

if prediction[0] == 0:
    print("\nâš ï¸ RECOMANDARE: Consultare oncolog urgentÄƒ + biopsie suplimentarÄƒ")
else:
    print("\nâœ… RECOMANDARE: Control de rutinÄƒ peste 6 luni")

# ========================================
# CROSS-VALIDATION CU PIPELINE
# ========================================

print("\n" + "=" * 60)
print("ðŸ”„ CROSS-VALIDATION CU PIPELINE (5-Fold)")
print("=" * 60)

# Cross-validation pe SVM pipeline
cv_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=5, scoring='accuracy')

print(f"\nðŸ“Š Scoruri pentru fiecare fold:")
for i, score in enumerate(cv_scores, 1):
    print(f"   Fold {i}: {score:.4f}")

print(f"\nâœ… Media: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
print(f"\nðŸ’¡ Interpretare:")
print(f"   - Modelul are ~{cv_scores.mean() * 100:.2f}% acurateÈ›e pe date nevÄƒzute")
print(f"   - VariaÈ›ie micÄƒ ({cv_scores.std():.4f}) = model stabil!")

print("""
\nðŸŽ“ DE CE CROSS-VALIDATION?
   - Un singur test set poate fi norocos/nenorocos
   - CV testeazÄƒ pe 5 pÄƒrÈ›i diferite â†’ estimare mai realistÄƒ
   - DetecteazÄƒ overfitting: dacÄƒ train score >> CV score
""")

