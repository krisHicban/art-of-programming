"""
ğŸ§  X È™i O cu Machine Learning - VERSIUNE INTEGRATÄ‚
==================================================
CombinÄƒ UI-ul original cu un sistem de antrenament Ã®mbunÄƒtÄƒÈ›it.

Caracteristici:
- InterfaÈ›Äƒ graficÄƒ prietenoasÄƒ construitÄƒ cu PyQt5.
- Agent de Machine Learning (Q-Learning) care Ã®nvaÈ›Äƒ sÄƒ joace.
- Sistem de antrenament avansat cu moduri multiple, inclusiv:
  - Curriculum Learning: Progresie de la adversari uÈ™ori la experÈ›i.
  - Experience Replay: ÃmbunÄƒtÄƒÈ›eÈ™te stabilitatea Ã®nvÄƒÈ›Äƒrii.
  - Epsilon Decay: EchilibreazÄƒ explorarea cu exploatarea.
- VitezÄƒ de antrenament optimizatÄƒ prin rularea pe un thread separat.
- Posibilitatea de a salva È™i Ã®ncÄƒrca progresul modelului AI.
- Vizualizarea Ã®n timp real a statisticilor È™i a procesului de Ã®nvÄƒÈ›are.
- Compatibilitate cu modelele vechi È™i noi.

Autori: Lao & Claude - Webinar "Arta ProgramÄƒrii cu AI"
"""

import sys
import random
import pickle
import numpy as np
from collections import defaultdict
import time
from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout,
                             QPushButton, QLabel, QDialog, QTextEdit,
                             QGridLayout, QSplitter, QProgressBar,
                             QComboBox, QSpinBox, QFileDialog)
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal


# -----------------------------------------------------------------------------
# CLASELE DE BAZÄ‚ PENTRU LOGICA AI
# -----------------------------------------------------------------------------

class ImprovedQLearningAgent:
    """
    ğŸ§  AGENTUL Q-LEARNING ÃMBUNÄ‚TÄ‚ÈšIT
    Versiune corectatÄƒ cu sistem de recompense, experience replay È™i euristici.
    """

    def __init__(self, learning_rate=0.1, discount_factor=0.95, epsilon=0.3):
        self.q_table = defaultdict(float)
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.initial_epsilon = epsilon

        # Statistici
        self.total_games = 0
        self.wins = 0
        self.losses = 0
        self.draws = 0

        # Pentru UI
        self.last_state = None
        self.last_action = None
        self.decision_reason = ""

        # Memory pentru experience replay
        self.memory = []
        self.max_memory = 1000

    def state_to_string(self, board):
        """ConverteÈ™te tabla Ã®ntr-un string pentru Q-table"""
        return ''.join(c if c != '' else '_' for c in board)

    def get_valid_actions(self, board):
        """GÄƒseÈ™te toate miÈ™cÄƒrile posibile"""
        return [i for i, cell in enumerate(board) if cell == '']

    def check_winner(self, board):
        """VerificÄƒ dacÄƒ cineva a cÃ¢È™tigat"""
        win_patterns = [
            [0, 1, 2], [3, 4, 5], [6, 7, 8],
            [0, 3, 6], [1, 4, 7], [2, 5, 8],
            [0, 4, 8], [2, 4, 6]
        ]
        for pattern in win_patterns:
            if board[pattern[0]] == board[pattern[1]] == board[pattern[2]] != '':
                return board[pattern[0]]
        return None

    def is_winning_move(self, board, position, player):
        """VerificÄƒ dacÄƒ o mutare cÃ¢È™tigÄƒ"""
        if board[position] != '':
            return False
        test_board = board[:]
        test_board[position] = player
        return self.check_winner(test_board) == player

    def is_blocking_move(self, board, position, player):
        """VerificÄƒ dacÄƒ o mutare blocheazÄƒ adversarul"""
        opponent = 'X' if player == 'O' else 'O'
        return self.is_winning_move(board, position, opponent)

    def evaluate_position(self, board, action, player='O'):
        """EvalueazÄƒ strategic o poziÈ›ie"""
        score = 0.0
        if self.is_winning_move(board, action, player):
            score += 1.0
        if self.is_blocking_move(board, action, player):
            score += 0.8
        if action == 4:
            score += 0.3
        elif action in [0, 2, 6, 8]:
            score += 0.2
        return score * 0.1

    def choose_action(self, board, training=True):
        """Alege o acÈ›iune folosind epsilon-greedy cu evaluare strategicÄƒ"""
        state = self.state_to_string(board)
        valid_actions = self.get_valid_actions(board)
        if not valid_actions:
            return None

        # VerificÄƒ Ã®ntÃ¢i mutÄƒrile critice (cÃ¢È™tig/blocare)
        for action in valid_actions:
            if self.is_winning_move(board, action, 'O'):
                self.decision_reason = f"ğŸ¯ CÃ‚È˜TIG: PoziÈ›ia {action + 1}"
                self.last_state = state
                self.last_action = action
                return action

        for action in valid_actions:
            if self.is_blocking_move(board, action, 'O'):
                self.decision_reason = f"ğŸ›¡ï¸ BLOCARE: PoziÈ›ia {action + 1}"
                self.last_state = state
                self.last_action = action
                return action

        # Epsilon-greedy pentru restul deciziilor
        epsilon = self.epsilon if training else max(0.05, self.epsilon * 0.1)
        if random.random() < epsilon and training:
            # Explorare cu preferinÈ›Äƒ pentru mutÄƒri strategice
            weights = [1.0 + self.evaluate_position(board, action) for action in valid_actions]
            action = random.choices(valid_actions, weights=weights, k=1)[0]
            self.decision_reason = f"ğŸ² EXPLORARE: PoziÈ›ia {action + 1} (Îµ={epsilon:.2f})"
        else:
            # Exploatare - foloseÈ™te Q-values + euristicÄƒ
            action_values = {a: self.q_table.get((state, a), 0.0) + self.evaluate_position(board, a) for a in
                             valid_actions}
            max_value = max(action_values.values())
            best_actions = [a for a, v in action_values.items() if abs(v - max_value) < 0.001]
            action = random.choice(best_actions)
            q_val = self.q_table.get((state, action), 0.0)
            self.decision_reason = f"ğŸ§  Q-LEARNING: PoziÈ›ia {action + 1} (Q={q_val:.3f})"

        self.last_state = state
        self.last_action = action
        return action

    def update_q_value(self, state, action, reward, next_state, done=False):
        """ActualizeazÄƒ Q-value folosind formula standard"""
        current_q = self.q_table.get((state, action), 0.0)
        if done:
            target = reward
        else:
            next_board = [c if c != '_' else '' for c in next_state]
            next_valid_actions = self.get_valid_actions(next_board)
            max_next_q = max((self.q_table.get((next_state, a), 0.0) for a in next_valid_actions), default=0.0)
            target = reward + self.discount_factor * max_next_q
        new_q = current_q + self.learning_rate * (target - current_q)
        self.q_table[(state, action)] = new_q
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.max_memory:
            self.memory.pop(0)

    def experience_replay(self, batch_size=32):
        """Re-antreneazÄƒ pe experienÈ›e anterioare"""
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in batch:
            self.update_q_value(state, action, reward, next_state, done)

    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):
        """Reduce epsilon gradual"""
        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)

    def game_ended(self, reward):
        """ActualizeazÄƒ statisticile la final de joc (pt UI)"""
        self.total_games += 1
        if reward > 0:
            self.wins += 1
        elif reward < 0:
            self.losses += 1
        else:
            self.draws += 1
        self.last_state = None
        self.last_action = None

    def get_statistics(self):
        """ReturneazÄƒ statisticile"""
        if self.total_games == 0:
            return {"total": 0, "wins": 0, "losses": 0, "draws": 0, "win_rate": 0, "loss_rate": 0, "draw_rate": 0}
        return {
            "total": self.total_games, "wins": self.wins, "losses": self.losses, "draws": self.draws,
            "win_rate": self.wins / self.total_games, "loss_rate": self.losses / self.total_games,
            "draw_rate": self.draws / self.total_games
        }

    def save_model(self, filename):
        """SalveazÄƒ modelul"""
        with open(filename, 'wb') as f:
            pickle.dump({
                'q_table': dict(self.q_table), 'stats': self.get_statistics(),
                'params': {'learning_rate': self.learning_rate, 'discount_factor': self.discount_factor,
                           'epsilon': self.epsilon},
                'memory': self.memory[-100:] if self.memory else []
            }, f)

    def load_model(self, filename):
        """ÃncarcÄƒ un model salvat"""
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
                self.q_table = defaultdict(float, data['q_table'])
                stats = data.get('stats', {})
                self.total_games, self.wins, self.losses, self.draws = stats.get('total', 0), stats.get('wins',
                                                                                                        0), stats.get(
                    'losses', 0), stats.get('draws', 0)
                if 'params' in data:
                    self.learning_rate = data['params'].get('learning_rate', self.learning_rate)
                    self.discount_factor = data['params'].get('discount_factor', self.discount_factor)
                    self.epsilon = data['params'].get('epsilon', self.epsilon)
                if 'memory' in data:
                    self.memory = data['memory']
                return True
        except Exception as e:
            print(f"Eroare la Ã®ncÄƒrcare: {e}")
            return False


class SmartOpponent:
    """Adversar inteligent pentru antrenament"""

    def __init__(self, difficulty=0.7):
        self.difficulty = difficulty
        self.agent = ImprovedQLearningAgent()  # Folosim logica de aici

    def choose_move(self, board):
        if random.random() < self.difficulty:
            # Mutare inteligentÄƒ
            valid_actions = self.agent.get_valid_actions(board)
            for p in ['X', 'O']:  # CÃ¢È™tigÄƒ dacÄƒ poate, blocheazÄƒ dacÄƒ trebuie
                for action in valid_actions:
                    if self.agent.is_winning_move(board, action, p):
                        return action
            if board[4] == '': return 4
            corners = [0, 2, 6, 8]
            random.shuffle(corners)
            for c in corners:
                if board[c] == '': return c
            return random.choice(valid_actions) if valid_actions else None
        else:
            # Mutare aleatorie
            valid_actions = self.agent.get_valid_actions(board)
            return random.choice(valid_actions) if valid_actions else None


class ImprovedTrainingThread(QThread):
    """Thread Ã®mbunÄƒtÄƒÈ›it pentru antrenament"""
    progress_updated = pyqtSignal(int, dict, str)
    training_completed = pyqtSignal()

    def __init__(self, agent, episodes=1000, training_mode='smart'):
        super().__init__()
        self.agent = agent
        self.episodes = episodes
        self.training_mode = training_mode
        self.should_stop = False
        self.opponents = {
            'random': SmartOpponent(0.0), 'easy': SmartOpponent(0.3),
            'medium': SmartOpponent(0.6), 'hard': SmartOpponent(0.9),
            'expert': SmartOpponent(1.0), 'curriculum': [SmartOpponent(0.1 * i) for i in range(11)]
        }

    def play_game(self, opponent):
        board = [''] * 9
        history = []
        turn = random.choice(['X', 'O'])
        while True:
            winner = self.agent.check_winner(board)
            if winner or '' not in board:
                reward_map = {'O': 1.0, 'X': -1.0}
                final_reward = reward_map.get(winner, 0.1)
                for state, action, next_state in reversed(history):
                    self.agent.update_q_value(state, action, final_reward, next_state, done=True)
                    final_reward *= -0.5  # PenalizeazÄƒ mutÄƒrile care duc la o pierdere
                self.agent.game_ended(1.0 if winner == 'O' else -1.0 if winner == 'X' else 0.0)
                break

            if turn == 'O':
                state = self.agent.state_to_string(board)
                action = self.agent.choose_action(board, training=True)
                if action is not None:
                    board[action] = 'O'
                    next_state = self.agent.state_to_string(board)
                    history.append((state, action, next_state))
                turn = 'X'
            else:
                move = opponent.choose_move(board)
                if move is not None:
                    board[move] = 'X'
                turn = 'O'

    def run(self):
        if self.training_mode == 'curriculum':
            levels = self.opponents['curriculum']
            episodes_per_level = self.episodes // len(levels)
            for i, opponent in enumerate(levels):
                if self.should_stop: break
                for episode in range(episodes_per_level):
                    if self.should_stop: break
                    self.play_game(opponent)
                    if episode % 10 == 0: self.agent.experience_replay(); self.agent.decay_epsilon()
                    if episode % 50 == 0:
                        progress = ((i * episodes_per_level) + episode) * 100 // self.episodes
                        status = f"Curriculum Nivel {i + 1}/{len(levels)} (Dificultate: {opponent.difficulty:.1f})"
                        self.progress_updated.emit(progress, self.agent.get_statistics(), status)
                        time.sleep(0.01)
        else:
            opponent = self.opponents.get(self.training_mode, self.opponents['medium'])
            for episode in range(self.episodes):
                if self.should_stop: break
                self.play_game(opponent)
                if episode % 10 == 0: self.agent.experience_replay(); self.agent.decay_epsilon()
                if episode % 50 == 0:
                    progress = episode * 100 // self.episodes
                    status = f"Antrenament vs '{self.training_mode}'"
                    self.progress_updated.emit(progress, self.agent.get_statistics(), status)
                    time.sleep(0.01)
        self.training_completed.emit()

    def stop_training(self):
        self.should_stop = True


# -----------------------------------------------------------------------------
# CLASA PRINCIPALÄ‚ PENTRU INTERFAÈšA GRAFICÄ‚
# -----------------------------------------------------------------------------

class TicTacToeAI(QWidget):
    """ğŸ® InterfaÈ›a principalÄƒ - versiune integratÄƒ"""

    def __init__(self):
        super().__init__()
        self.ml_agent = ImprovedQLearningAgent()
        self.training_thread = None
        if self.ml_agent.load_model('xo_ai_model_improved.pkl'):
            print("âœ… Model Ã®mbunÄƒtÄƒÈ›it Ã®ncÄƒrcat!")
        elif self.ml_agent.load_model('xo_ai_model.pkl'):
            print("âš ï¸ Model vechi Ã®ncÄƒrcat - considerÄƒ re-antrenarea!")
        else:
            print("â„¹ï¸ Model nou creat - Ã®ncepe antrenamentul!")
        self.initUI()

    def initUI(self):
        self.setWindowTitle('ğŸ§  X È™i O - Machine Learning (Versiune ÃmbunÄƒtÄƒÈ›itÄƒ)')
        self.setGeometry(50, 50, 1000, 700)
        main_layout = QHBoxLayout()
        splitter = QSplitter(Qt.Horizontal)
        game_widget = self.create_game_panel()
        training_widget = self.create_improved_training_panel()
        splitter.addWidget(game_widget)
        splitter.addWidget(training_widget)
        splitter.setSizes([450, 550])
        main_layout.addWidget(splitter)
        self.setLayout(main_layout)
        self.reset_game()

    def create_game_panel(self):
        widget = QWidget()
        layout = QVBoxLayout()
        title = QLabel('ğŸ¯ JoacÄƒ Ã®mpotriva AI-ului Antrenat!')
        title.setAlignment(Qt.AlignCenter)
        title.setStyleSheet("font-size: 18px; font-weight: bold; color: #2c3e50; margin: 10px;")
        layout.addWidget(title)
        self.status_label = QLabel()
        self.status_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.status_label)
        self.thinking_label = QLabel()
        self.thinking_label.setAlignment(Qt.AlignCenter)
        self.thinking_label.setStyleSheet(
            "font-size: 14px; background-color: #f39c12; color: white; padding: 8px; border-radius: 5px; font-weight: bold;")
        self.thinking_label.hide()
        layout.addWidget(self.thinking_label)
        grid_container = QWidget()
        grid_layout = QGridLayout(grid_container)
        grid_layout.setSpacing(10)
        self.buttons = []
        for i in range(9):
            button = QPushButton('')
            button.setFixedSize(100, 100)
            index = i
            button.clicked.connect(lambda _, idx=index: self.on_button_click(idx))
            self.buttons.append(button)
            grid_layout.addWidget(button, i // 3, i % 3)
        layout.addWidget(grid_container, 0, Qt.AlignCenter)
        self.new_game_btn = QPushButton('ğŸ”„ Joc Nou')
        self.new_game_btn.clicked.connect(self.reset_game)
        self.new_game_btn.setStyleSheet("padding: 10px; font-size: 14px;")
        layout.addWidget(self.new_game_btn, 0, Qt.AlignCenter)
        layout.addStretch()
        widget.setLayout(layout)
        return widget

    def create_improved_training_panel(self):
        widget = QWidget()
        layout = QVBoxLayout()
        title = QLabel('ğŸ“ Antrenament Avansat')
        title.setAlignment(Qt.AlignCenter)
        title.setStyleSheet("font-size: 16px; font-weight: bold; color: #2c3e50;")
        layout.addWidget(title)
        self.stats_label = QLabel(self.get_stats_text())
        self.stats_label.setStyleSheet(
            "background-color: #f8f9fa; padding: 10px; border-radius: 5px; border: 1px solid #dee2e6; font-family: 'Courier New', monospace;")
        layout.addWidget(self.stats_label)
        mode_layout = QHBoxLayout()
        mode_layout.addWidget(QLabel("Mod antrenament:"))
        self.training_mode = QComboBox()
        self.training_mode.addItems(['curriculum', 'random', 'easy', 'medium', 'hard', 'expert'])
        self.training_mode.setToolTip(
            "â€¢ curriculum: Progresie gradualÄƒ (recomandat!)\nâ€¢ random: Adversar complet aleatoriu\nâ€¢ easy/medium/hard: Dificultate fixÄƒ\nâ€¢ expert: Adversar perfect")
        mode_layout.addWidget(self.training_mode)
        layout.addLayout(mode_layout)
        episodes_layout = QHBoxLayout()
        episodes_layout.addWidget(QLabel("Episoade:"))
        self.episodes_spin = QSpinBox()
        self.episodes_spin.setRange(100, 100000)
        self.episodes_spin.setValue(5000)
        self.episodes_spin.setSingleStep(100)
        episodes_layout.addWidget(self.episodes_spin)
        layout.addLayout(episodes_layout)
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        layout.addWidget(self.progress_bar)
        self.training_status = QLabel('')
        self.training_status.setAlignment(Qt.AlignCenter)
        self.training_status.setStyleSheet("color: #e67e22; font-weight: bold;")
        layout.addWidget(self.training_status)
        training_btns = QHBoxLayout()
        self.train_btn = QPushButton('ğŸƒ AntreneazÄƒ')
        self.train_btn.clicked.connect(self.start_training)
        self.train_btn.setStyleSheet(
            "background-color: #27ae60; color: white; padding: 8px; border-radius: 4px; font-weight: bold;")
        training_btns.addWidget(self.train_btn)
        self.stop_btn = QPushButton('â¹ï¸ Stop')
        self.stop_btn.clicked.connect(self.stop_training)
        self.stop_btn.setEnabled(False)
        training_btns.addWidget(self.stop_btn)
        layout.addLayout(training_btns)
        model_btns = QHBoxLayout()
        save_btn = QPushButton('ğŸ’¾ SalveazÄƒ')
        save_btn.clicked.connect(self.save_model)
        model_btns.addWidget(save_btn)
        load_btn = QPushButton('ğŸ“ ÃncarcÄƒ')
        load_btn.clicked.connect(self.load_model)
        model_btns.addWidget(load_btn)
        layout.addLayout(model_btns)
        info = QLabel("ğŸ“Š PARAMETRI: Learning rate: 0.1, Discount: 0.95, Epsilon: variabil\n"
                      "ğŸ¯ SFATURI:\n"
                      "1. FoloseÈ™te 'curriculum' pentru cel mai bun rezultat.\n"
                      "2. 5000+ episoade pentru performanÈ›Äƒ bunÄƒ.\n"
                      "3. SalveazÄƒ dupÄƒ fiecare sesiune de antrenament.")
        info.setStyleSheet("""
            background-color: #e8f5e9; padding: 10px; border-radius: 5px;
            font-size: 11px; border-left: 4px solid #4caf50;""")
        info.setWordWrap(True)
        layout.addWidget(info)
        layout.addStretch()
        widget.setLayout(layout)
        return widget

    def get_stats_text(self):
        stats = self.ml_agent.get_statistics()
        epsilon_text = f"{self.ml_agent.epsilon:.3f}" if hasattr(self.ml_agent, 'epsilon') else "N/A"
        return f"""
ğŸ“Š STATISTICI MODEL:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Jocuri totale: {stats['total']:,}
Victorii: {stats['wins']:,} ({stats['win_rate']:.1%})
ÃnfrÃ¢ngeri: {stats['losses']:,} ({stats['loss_rate']:.1%})
EgalitÄƒÈ›i: {stats['draws']:,} ({stats['draw_rate']:.1%})

Q-Table: {len(self.ml_agent.q_table):,} intrÄƒri
Epsilon: {epsilon_text}
Memorie: {len(self.ml_agent.memory)} experienÈ›e"""

    def reset_game(self):
        self.board = [''] * 9
        self.player_turn = True
        self.game_over = False
        self.last_ai_move = None
        for button in self.buttons:
            button.setText('')
            button.setEnabled(True)
            button.setStyleSheet("""
                QPushButton {font-size: 40px; font-weight: bold; border: 2px solid #34495e; border-radius: 8px; background-color: #ecf0f1;}
                QPushButton:hover {background-color: #bdc3c7;}""")
        self.status_label.setText('RÃ¢ndul tÄƒu (X)')
        self.status_label.setStyleSheet(
            "font-size: 16px; font-weight: bold; background-color: #3498db; color: white; padding: 10px; border-radius: 5px;")
        self.thinking_label.hide()

    def on_button_click(self, index):
        if not self.player_turn or self.game_over or self.board[index] != '': return
        self.board[index] = 'X'
        self.buttons[index].setText('X')
        self.buttons[index].setStyleSheet(
            "font-size: 40px; font-weight: bold; border: 2px solid #34495e; border-radius: 8px; background-color: #3498db; color: white;")
        if self.check_game_end(): return
        self.player_turn = False
        self.status_label.setText('ğŸ¤– AI-ul se gÃ¢ndeÈ™te...')
        self.thinking_label.setText('ğŸ§  Analizez poziÈ›iile...')
        self.thinking_label.show()
        QTimer.singleShot(800, self.ai_move)

    def ai_move(self):
        action = self.ml_agent.choose_action(self.board, training=False)
        if action is not None:
            self.board[action] = 'O'
            self.buttons[action].setText('O')
            self.buttons[action].setStyleSheet(
                "font-size: 40px; font-weight: bold; border: 2px solid #34495e; border-radius: 8px; background-color: #e74c3c; color: white;")
            self.last_ai_move = action
            self.thinking_label.setText(self.ml_agent.decision_reason)
            QTimer.singleShot(1500, self.thinking_label.hide)
        if not self.check_game_end():
            self.player_turn = True
            self.status_label.setText('RÃ¢ndul tÄƒu (X)')

    def check_game_end(self):
        winner = self.check_winner()
        if winner:
            self.game_over = True
            self.thinking_label.hide()
            if winner == 'X':
                self.status_label.setText('ğŸ‰ Ai cÃ¢È™tigat!')
                self.status_label.setStyleSheet(
                    "font-size: 16px; font-weight: bold; background-color: #27ae60; color: white; padding: 10px; border-radius: 5px;")
            else:
                self.status_label.setText('ğŸ¤– AI-ul a cÃ¢È™tigat!')
                self.status_label.setStyleSheet(
                    "font-size: 16px; font-weight: bold; background-color: #e74c3c; color: white; padding: 10px; border-radius: 5px;")
            for button in self.buttons: button.setEnabled(False)
            self.stats_label.setText(self.get_stats_text())
            return True
        elif '' not in self.board:
            self.game_over = True
            self.status_label.setText('ğŸ¤ Egalitate!')
            self.status_label.setStyleSheet(
                "font-size: 16px; font-weight: bold; background-color: #f39c12; color: white; padding: 10px; border-radius: 5px;")
            self.thinking_label.hide()
            for button in self.buttons: button.setEnabled(False)
            self.stats_label.setText(self.get_stats_text())
            return True
        return False

    def check_winner(self):
        win_conditions = [[0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]
        for cond in win_conditions:
            if self.board[cond[0]] == self.board[cond[1]] == self.board[cond[2]] != '':
                for idx in cond: self.buttons[idx].setStyleSheet(
                    self.buttons[idx].styleSheet() + "border: 4px solid #f1c40f;")
                return self.board[cond[0]]
        return None

    def start_training(self):
        self.training_thread = ImprovedTrainingThread(self.ml_agent, self.episodes_spin.value(),
                                                      self.training_mode.currentText())
        self.training_thread.progress_updated.connect(self.update_training_progress)
        self.training_thread.training_completed.connect(self.training_finished)
        self.train_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.training_status.setText(f"Antrenament {self.training_mode.currentText()}...")
        self.training_thread.start()

    def update_training_progress(self, progress, stats, status):
        self.progress_bar.setValue(progress)
        self.stats_label.setText(self.get_stats_text())
        self.training_status.setText(status)

    def training_finished(self):
        self.train_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.progress_bar.setVisible(False)
        self.training_status.setText("âœ… Antrenament complet!")
        self.ml_agent.save_model('xo_ai_model_improved.pkl')
        QTimer.singleShot(3000, lambda: self.training_status.setText(""))

    def stop_training(self):
        if self.training_thread:
            self.training_thread.stop_training()
            self.training_status.setText("â¹ï¸ Antrenament oprit...")

    def save_model(self):
        filename, _ = QFileDialog.getSaveFileName(self, "SalveazÄƒ Model", "xo_ai_model_improved.pkl",
                                                  "Pickle Files (*.pkl)")
        if filename:
            self.ml_agent.save_model(filename)
            self.training_status.setText("ğŸ’¾ Model salvat!")
            QTimer.singleShot(2000, lambda: self.training_status.setText(""))

    def load_model(self):
        filename, _ = QFileDialog.getOpenFileName(self, "ÃncarcÄƒ Model", "", "Pickle Files (*.pkl)")
        if filename and self.ml_agent.load_model(filename):
            self.stats_label.setText(self.get_stats_text())
            self.training_status.setText("ğŸ“ Model Ã®ncÄƒrcat!")
            QTimer.singleShot(2000, lambda: self.training_status.setText(""))
        elif filename:
            self.training_status.setText("âŒ Eroare la Ã®ncÄƒrcare!")


# -----------------------------------------------------------------------------
# BLOCUL DE EXECUÈšIE PRINCIPAL
# -----------------------------------------------------------------------------

if __name__ == '__main__':
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ğŸ§  X È™i O cu Machine Learning - Versiune ÃmbunÄƒtÄƒÈ›itÄƒ     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    ğŸ¯ CARACTERISTICI NOI:
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    âœ… Sistem de recompense Ã®mbunÄƒtÄƒÈ›it
    âœ… Antrenament cu adversari inteligenÈ›i (Curriculum Learning)
    âœ… Experience Replay pentru stabilitate
    âœ… Epsilon Decay automat
    âœ… Compatibil cu modele vechi È™i noi

    ğŸ“Š MODURI DE ANTRENAMENT:
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â€¢ CURRICULUM: Cel mai bun! Progresie de la uÈ™or la expert.
    â€¢ RANDOM/EASY/MEDIUM/HARD/EXPERT: Dificultate fixÄƒ.

    ğŸ’¡ RECOMANDÄ‚RI:
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    1. FoloseÈ™te modul CURRICULUM pentru 5000+ episoade.
    2. SalveazÄƒ dupÄƒ fiecare sesiune de antrenament.
    3. UrmÄƒreÈ™te rata de victorii - È›inta este >70% vs 'medium'.

    Mult succes! ğŸ®
    """)
    app = QApplication(sys.argv)
    game = TicTacToeAI()
    game.show()
    sys.exit(app.exec_())