# Tema 15: Introducere Ã®n ReÈ›ele Neuronale cu NumPy

**TemÄƒ comunÄƒ pentru Ã®nÈ›elegerea conceptelor de bazÄƒ ale reÈ›elelor neuronale**

---

## ğŸ“š Ghid de Abordare - Research First, Code Second

### Filosofia acestei teme: **Don't just Code - but Evolve your Knowledge**

AceastÄƒ temÄƒ urmÄƒreÈ™te **douÄƒ obiective paralele**:
1. **StÄƒpÃ¢nirea NumPy** - Ã®nÈ›elegerea operaÈ›iilor vectorizate, broadcasting, È™i manipularea eficientÄƒ a matricelor
2. **Fundamentele ReÈ›elelor Neuronale** - Ã®nÈ›elegerea conceptualÄƒ a cum "Ã®nvaÈ›Äƒ" o maÈ™inÄƒ

### ğŸ” Metodologia de lucru recomandatÄƒ

Pentru **FIECARE** exerciÈ›iu, urmaÈ›i aceastÄƒ abordare Ã®n 4 paÈ™i:

#### Pasul 1: Research (15-30 minute per exerciÈ›iu)
Ãnainte de a scrie orice linie de cod, cercetaÈ›i:
- **Ce este conceptul?** (ex: Ce este un neuron? De ce ReLU?)
- **De ce existÄƒ?** (Ce problemÄƒ rezolvÄƒ? Care e intuiÈ›ia?)
- **Cum funcÈ›ioneazÄƒ matematic?** (Formulele, dar È™i intuiÈ›ia din spate)
- **Exemple vizuale** (CÄƒutaÈ›i grafice, animaÈ›ii, diagrame)

**Resurse de start pentru research:**
- 3Blue1Brown - Neural Network series (pentru intuiÈ›ie vizualÄƒ)
- Papers with Code (pentru implementÄƒri practice)
- NumPy documentation (pentru operaÈ›ii specifice)

#### Pasul 2: Implementare (20-30 minute)
- Acum doar Ã®ncepeÈ›i sÄƒ scrieÈ›i codul
- ComentaÈ›i FIECARE linie cu ce face È˜I de ce
- VerificaÈ›i dimensiunile matricelor la fiecare pas

#### Pasul 3: ReflecÈ›ie È™i Documentare (10-15 minute)
- ScrieÈ›i un paragraf despre ce aÈ›i Ã®nvÄƒÈ›at
- NotaÈ›i 2-3 Ã®ntrebÄƒri noi care v-au apÄƒrut
- DocumentaÈ›i orice "aha!" moment

### ğŸ¯ Exemple de Ã®ntrebÄƒri pentru research

**Pentru ExerciÈ›iul 1 (Neuron cu ReLU):**
- De ce ReLU È™i nu o funcÈ›ie liniarÄƒ? (hint: non-linearitate)
- Ce se Ã®ntÃ¢mplÄƒ cu "dying ReLU problem"?
- Cum aratÄƒ ReLU vs Sigmoid vs Tanh vizual?
- Ce reprezintÄƒ weights È™i bias Ã®n lumea realÄƒ?

**Pentru ExerciÈ›iul 2 (Sigmoid):**
- De ce sigmoid mapeazÄƒ Ã®n (0,1)? Pentru ce e util asta?
- Care e derivata lui sigmoid È™i de ce e importantÄƒ?
- Ce e "vanishing gradient problem"?
- CÃ¢nd folosim sigmoid vs softmax?

**Pentru ExerciÈ›iul 3 (ReÈ›ea cu 2 straturi):**
- Ce Ã®nseamnÄƒ "fully connected"? 
- De ce avem nevoie de straturi multiple?
- Ce e forward propagation vs backward propagation?
- Cum aleg numÄƒrul de neuroni Ã®n stratul ascuns?

### ğŸ’¡ Anti-patterns de evitat

âŒ **Nu faceÈ›i:** Copy-paste din ChatGPT fÄƒrÄƒ Ã®nÈ›elegere
âœ… **FaceÈ›i:** FolosiÈ›i AI pentru clarificÄƒri, apoi implementaÈ›i singuri

âŒ **Nu faceÈ›i:** SÄƒriÈ›i direct la cod
âœ… **FaceÈ›i:** DesenaÈ›i pe hÃ¢rtie ce vreÈ›i sÄƒ faceÈ›i Ã®ntÃ¢i

âŒ **Nu faceÈ›i:** ImplementaÈ›i tot deodatÄƒ
âœ… **FaceÈ›i:** Baby steps - testaÈ›i fiecare funcÈ›ie izolat

### ğŸ“Š Ce Ã®nseamnÄƒ "Ã®nÈ›elegere profundÄƒ"

È˜tiÈ›i cÄƒ aÈ›i Ã®nÈ›eles cu adevÄƒrat cÃ¢nd puteÈ›i:
1. Explica conceptul unui coleg Ã®n cuvinte simple
2. Desena pe tablÄƒ cum circulÄƒ datele prin neuron/reÈ›ea
3. Prezice ce se Ã®ntÃ¢mplÄƒ dacÄƒ schimbaÈ›i un parametru
4. Identifica cÃ¢nd È™i de ce ar eÈ™ua implementarea

### ğŸ”— Conexiunea NumPy - Neural Networks

Ãn timp ce lucraÈ›i, observaÈ›i:
- **Dot product** (`np.dot`) = suma ponderatÄƒ Ã®n neuroni
- **Broadcasting** = aplicare eficientÄƒ a bias-ului
- **Vectorizare** = procesare batch (mai multe exemple simultan)
- **Reshape** = pregÄƒtirea datelor pentru layere diferite

---

## I. ImplementeazÄƒ un neuron simplu cu activare ReLU

CreeazÄƒ o clasÄƒ `NeuronSimplu` care sÄƒ simuleze comportamentul unui neuron cu funcÈ›ia de activare ReLU.

```python
import numpy as np

class NeuronSimplu:
    def __init__(self, numar_intrari):
        # IniÈ›ializeazÄƒ weights È™i bias aleatoriu
        self.weights = np.random.randn(numar_intrari)
        self.bias = np.random.randn()
    
    def relu(self, x):
        # ImplementeazÄƒ funcÈ›ia ReLU
        # ReLU(x) = max(0, x)
        pass
    
    def forward(self, intrari):
        # CalculeazÄƒ output = ReLU(weights Â· intrari + bias)
        pass

# Testare
neuron = NeuronSimplu(3)
intrare_test = np.array([1.0, 2.0, -0.5])
output = neuron.forward(intrare_test)
print(f"Output neuron: {output}")
```

**CerinÈ›e:**
- FuncÈ›ia `relu()` trebuie sÄƒ returneze 0 pentru valori negative È™i valoarea Ã®nsÄƒÈ™i pentru valori pozitive
- Metoda `forward()` trebuie sÄƒ calculeze suma ponderatÄƒ È™i sÄƒ aplice ReLU
- AfiÈ™eazÄƒ weights, bias È™i output-ul final

## II. CreeazÄƒ È™i aplicÄƒ funcÈ›ia sigmoid pe un dataset aleatoriu

ImplementeazÄƒ funcÈ›ia sigmoid È™i aplicÄƒ-o pe 100 de valori generate aleatoriu.

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    # ImplementeazÄƒ funcÈ›ia sigmoid: f(x) = 1 / (1 + e^(-x))
    pass

# GenereazÄƒ 100 de valori aleatorii Ã®ntre -10 È™i 10
date_aleatorii = np.random.uniform(-10, 10, 100)

# AplicÄƒ sigmoid pe toate valorile
rezultate = date_aleatorii # aplicÄƒ sigmoid aici

# Vizualizare (opÈ›ional dar recomandat)
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.hist(date_aleatorii, bins=20, alpha=0.7, color='blue')
plt.title('DistribuÈ›ia valorilor iniÈ›iale')

plt.subplot(1, 2, 2)
plt.hist(rezultate, bins=20, alpha=0.7, color='green')
plt.title('DistribuÈ›ia dupÄƒ aplicarea sigmoid')
plt.show()

# Statistici
print(f"Media iniÈ›ialÄƒ: {np.mean(date_aleatorii):.3f}")
print(f"Media dupÄƒ sigmoid: {np.mean(rezultate):.3f}")
print(f"Min/Max iniÈ›ial: {np.min(date_aleatorii):.3f}/{np.max(date_aleatorii):.3f}")
print(f"Min/Max dupÄƒ sigmoid: {np.min(rezultate):.3f}/{np.max(rezultate):.3f}")
```

**CerinÈ›e:**
- FuncÈ›ia sigmoid trebuie sÄƒ mapeze orice valoare realÄƒ Ã®n intervalul (0, 1)
- CalculeazÄƒ È™i afiÈ™eazÄƒ media, minimul È™i maximul Ã®nainte È™i dupÄƒ aplicarea sigmoid
- ObservÄƒ cum sigmoid "comprimÄƒ" valorile Ã®n intervalul (0, 1)

## III. SimuleazÄƒ o reÈ›ea neuronalÄƒ cu 2 straturi complet vectorizatÄƒ

ImplementeazÄƒ o reÈ›ea neuronalÄƒ simplÄƒ cu 2 straturi dense (fully connected) folosind doar NumPy È™i operaÈ›ii vectorizate.

```python
import numpy as np

class ReteasNeuronala:
    def __init__(self, dim_intrare, dim_ascuns, dim_iesire):
        # IniÈ›ializare parametri pentru 2 straturi
        # Stratul 1: dim_intrare -> dim_ascuns
        self.W1 = np.random.randn(dim_intrare, dim_ascuns) * 0.1
        self.b1 = np.zeros((1, dim_ascuns))
        
        # Stratul 2: dim_ascuns -> dim_iesire
        self.W2 = np.random.randn(dim_ascuns, dim_iesire) * 0.1
        self.b2 = np.zeros((1, dim_iesire))
    
    def relu(self, Z):
        # ImplementeazÄƒ ReLU vectorizat
        pass
    
    def sigmoid(self, Z):
        # ImplementeazÄƒ sigmoid vectorizat
        pass
    
    def forward(self, X):
        """
        Forward pass prin reÈ›ea
        X: matrice de intrare (batch_size, dim_intrare)
        """
        # Stratul 1 cu activare ReLU
        Z1 = # calculeazÄƒ X Â· W1 + b1
        A1 = # aplicÄƒ ReLU
        
        # Stratul 2 cu activare sigmoid
        Z2 = # calculeazÄƒ A1 Â· W2 + b2
        A2 = # aplicÄƒ sigmoid
        
        return A2
    
    def prezice(self, X):
        """ReturneazÄƒ predicÈ›ii binare (0 sau 1)"""
        output = self.forward(X)
        return (output > 0.5).astype(int)

# Testare cu date simulate
np.random.seed(42)

# CreeazÄƒ o reÈ›ea: 4 intrÄƒri -> 5 neuroni ascunÈ™i -> 2 ieÈ™iri
retea = ReteasNeuronala(dim_intrare=4, dim_ascuns=5, dim_iesire=2)

# Date de test: 10 exemple cu 4 features fiecare
X_test = np.random.randn(10, 4)

# ObÈ›ine predicÈ›iile
predictii = retea.forward(X_test)
clase_prezise = retea.prezice(X_test)

print("Forma intrÄƒrii:", X_test.shape)
print("Forma ieÈ™irii:", predictii.shape)
print("\nPrimele 3 probabilitÄƒÈ›i de ieÈ™ire:")
print(predictii[:3])
print("\nPrimele 3 clase prezise:")
print(clase_prezise[:3])

# VerificÄƒ vectorizarea
print(f"\nDimensiuni parametri:")
print(f"W1: {retea.W1.shape}, b1: {retea.b1.shape}")
print(f"W2: {retea.W2.shape}, b2: {retea.b2.shape}")
```

**CerinÈ›e:**
- Toate operaÈ›iile trebuie sÄƒ fie **complet vectorizate** (fÄƒrÄƒ bucle for explicite)
- Stratul 1 foloseÈ™te activare ReLU, stratul 2 foloseÈ™te sigmoid
- ReÈ›eaua trebuie sÄƒ poatÄƒ procesa un batch de mai multe exemple simultan
- AfiÈ™eazÄƒ dimensiunile tuturor matricelor pentru a verifica corectitudinea
- Metoda `prezice()` trebuie sÄƒ returneze clase binare (0 sau 1) bazate pe un prag de 0.5
[
## Criterii de evaluare

1. **Corectitudinea implementÄƒrii** (40%)
   - FuncÈ›iile matematice sunt implementate corect
   - OperaÈ›iile matriceale sunt fÄƒcute corect

2. **Vectorizare** (30%)
   - Codul foloseÈ™te operaÈ›ii NumPy vectorizate
   - Nu existÄƒ bucle for inutile

3. **Claritatea codului** (20%)
   - Cod bine comentat È™i organizat
   - Variabile cu nume sugestive

4. **Testing È™i validare** (10%)
   - Testarea funcÈ›iilor cu date de exemplu]()
   - AfiÈ™area rezultatelor Ã®ntr-un mod clar

## Note importante

- FolosiÈ›i `np.random.seed()` pentru reproducibilitate
- AtenÈ›ie la dimensiunile matricelor Ã®n operaÈ›iile matriceale
- Broadcasting-ul NumPy vÄƒ poate ajuta sÄƒ evitaÈ›i bucle
- Pentru debugging, afiÈ™aÈ›i formele (shapes) matricelor intermediare

## Resurse utile

- [NumPy Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)
- [FuncÈ›ii de activare](https://en.wikipedia.org/wiki/Activation_function)
- [Vectorizare Ã®n NumPy](https://numpy.org/doc/stable/user/quickstart.html#vectorization)

**Format livrare:** Jupyter Notebook sau script Python cu output-ul rulÄƒrii