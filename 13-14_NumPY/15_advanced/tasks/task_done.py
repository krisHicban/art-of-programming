"""
Tema 15: Introducere Ã®n ReÈ›ele Neuronale cu NumPy
===================================================
Implementare completÄƒ cu explicaÈ›ii detaliate È™i research notes

Date: 2025
"""

import numpy as np
import matplotlib.pyplot as plt

# SetÄƒm seed pentru reproducibilitate
np.random.seed(42)

print("=" * 80)
print("TEMA 15: INTRODUCERE ÃN REÈšELE NEURONALE CU NUMPY")
print("=" * 80)

# =============================================================================
# EXERCIÈšIUL 1: Neuron Simplu cu Activare ReLU
# =============================================================================

print("\n" + "=" * 80)
print("EXERCIÈšIUL 1: Neuron Simplu cu Activare ReLU")
print("=" * 80)

"""
Research Notes - Ce este un neuron?
-----------------------------------
Un neuron artificial simuleazÄƒ comportamentul unui neuron biologic:
- PrimeÈ™te mai multe intrÄƒri (dendrite)
- Fiecare intrare are o pondere (weight) - importanÈ›a semnalului
- Se adaugÄƒ un bias (prag de activare)
- Se aplicÄƒ o funcÈ›ie de activare (ReLU Ã®n cazul nostru)
- Produce un output

De ce ReLU?
-----------
ReLU (Rectified Linear Unit) = max(0, x)
- Simplu de calculat È™i derivat
- EvitÄƒ problema "vanishing gradient" (faÈ›Äƒ de sigmoid/tanh)
- Introduce non-linearitate (esenÈ›ial pentru Ã®nvÄƒÈ›are complexÄƒ)
- Biologic plauzibil (neuronii fie se activeazÄƒ, fie nu)

Problema "Dying ReLU":
- DacÄƒ neuronul primeÈ™te valori negative constant, gradientul devine 0
- Neuronul "moare" È™i nu mai Ã®nvaÈ›Äƒ
- SoluÈ›ii: Leaky ReLU, ELU, etc.
"""


class NeuronSimplu:
    def __init__(self, numar_intrari):
        """
        IniÈ›ializeazÄƒ un neuron cu parametri aleatorii

        Parameters:
        -----------
        numar_intrari : int
            NumÄƒrul de conexiuni de intrare ale neuronului
        """
        # IniÈ›ializÄƒm weights folosind distribuÈ›ia normalÄƒ standard
        # MultiplicÄƒm cu 0.1 pentru valori mici iniÈ›iale (evitÄƒ saturarea)
        self.weights = np.random.randn(numar_intrari) * 0.1

        # Bias-ul Ã®ncepe de la o valoare micÄƒ aleatorie
        # ReprezintÄƒ pragul de activare al neuronului
        self.bias = np.random.randn() * 0.1

        print(f"Neuron iniÈ›ializat cu {numar_intrari} intrÄƒri")
        print(f"Weights iniÈ›iale: {self.weights}")
        print(f"Bias iniÈ›ial: {self.bias:.4f}")

    def relu(self, x):
        """
        ImplementeazÄƒ funcÈ›ia de activare ReLU
        ReLU(x) = max(0, x)

        IntuiÈ›ie: DacÄƒ suma ponderatÄƒ e negativÄƒ, neuronul nu se activeazÄƒ (output 0)
                  DacÄƒ e pozitivÄƒ, transmite semnalul proporÈ›ional cu intensitatea
        """
        return np.maximum(0, x)

    def forward(self, intrari):
        """
        CalculeazÄƒ output-ul neuronului (forward pass)

        Formula: output = ReLU(Î£(wi * xi) + b)
        unde:
        - wi = weight pentru intrarea i
        - xi = valoarea intrÄƒrii i
        - b = bias

        Parameters:
        -----------
        intrari : numpy.ndarray
            Vector cu valorile de intrare

        Returns:
        --------
        float
            Output-ul neuronului dupÄƒ aplicarea ReLU
        """
        # Pasul 1: CalculÄƒm suma ponderatÄƒ (dot product)
        # Aceasta reprezintÄƒ "potenÈ›ialul" neuronului
        suma_ponderata = np.dot(self.weights, intrari) + self.bias

        # Debugging: afiÈ™Äƒm calculul intermediar
        print(f"\nCalcul detaliat:")
        print(f"IntrÄƒri: {intrari}")
        print(f"Weights: {self.weights}")
        print(f"Suma ponderatÄƒ (Ã®nainte de bias): {np.dot(self.weights, intrari):.4f}")
        print(f"Suma ponderatÄƒ (cu bias): {suma_ponderata:.4f}")

        # Pasul 2: AplicÄƒm funcÈ›ia de activare ReLU
        output = self.relu(suma_ponderata)

        print(f"Output dupÄƒ ReLU: {output:.4f}")

        return output


# Testare ExerciÈ›iul 1
print("\nTESTARE NEURON SIMPLU:")
print("-" * 40)

neuron = NeuronSimplu(3)
intrare_test = np.array([1.0, 2.0, -0.5])

print(f"\nIntrare de test: {intrare_test}")
output = neuron.forward(intrare_test)

print(f"\n{'=' * 40}")
print(f"OUTPUT FINAL NEURON: {output:.4f}")
print(f"{'=' * 40}")

# Test adiÈ›ional cu valori care vor produce output negativ (pentru a vedea ReLU Ã®n acÈ›iune)
print("\nTest cu valori negative mari:")
intrare_negativa = np.array([-5.0, -3.0, -2.0])
output_negativ = neuron.forward(intrare_negativa)
print(f"Output pentru intrÄƒri negative: {output_negativ:.4f} (demonstreazÄƒ efectul ReLU)")

# =============================================================================
# EXERCIÈšIUL 2: FuncÈ›ia Sigmoid pe Dataset Aleatoriu
# =============================================================================

print("\n" + "=" * 80)
print("EXERCIÈšIUL 2: FuncÈ›ia Sigmoid pe Dataset Aleatoriu")
print("=" * 80)

"""
Research Notes - FuncÈ›ia Sigmoid
---------------------------------
Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))

Caracteristici:
- MapeazÄƒ orice valoare realÄƒ Ã®n intervalul (0, 1)
- FormÄƒ de "S" - smooth È™i diferenÈ›iabilÄƒ peste tot
- Derivata: Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x)) - convenabilÄƒ pentru backpropagation
- Output interpretabil ca probabilitate

UtilizÄƒri:
- Output layer pentru clasificare binarÄƒ
- Gates Ã®n LSTM/GRU
- Istoric: foarte popularÄƒ Ã®nainte de ReLU

Probleme:
- Vanishing gradient: pentru |x| mare, gradientul â†’ 0
- Computational expensive (exponenÈ›ialÄƒ)
- Output-ul nu e zero-centered
"""


def sigmoid(x):
    """
    ImplementeazÄƒ funcÈ›ia sigmoid: f(x) = 1 / (1 + e^(-x))

    VectorizatÄƒ - funcÈ›ioneazÄƒ pe scalari, vectori È™i matrici

    Pentru valori foarte mari/mici, folosim np.clip pentru stabilitate numericÄƒ
    """
    # EvitÄƒm overflow pentru valori foarte mari negative
    # Clipping la [-500, 500] previne exp() overflow
    x_safe = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_safe))


# DemonstraÈ›ie vizualÄƒ a funcÈ›iei sigmoid
print("\nVIZUALIZARE FUNCÈšIA SIGMOID:")
print("-" * 40)

# CreÄƒm un range de valori pentru a vedea forma sigmoid
x_demo = np.linspace(-10, 10, 1000)
y_sigmoid = sigmoid(x_demo)

plt.figure(figsize=(12, 4))

# Subplot 1: FuncÈ›ia sigmoid
plt.subplot(1, 3, 1)
plt.plot(x_demo, y_sigmoid, 'b-', linewidth=2, label='Sigmoid')
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='y=0.5')
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
plt.grid(True, alpha=0.3)
plt.xlabel('x')
plt.ylabel('Ïƒ(x)')
plt.title('FuncÈ›ia Sigmoid')
plt.legend()
plt.ylim(-0.1, 1.1)

# Subplot 2: ComparaÈ›ie cu ReLU
plt.subplot(1, 3, 2)
y_relu = np.maximum(0, x_demo)
plt.plot(x_demo, y_sigmoid, 'b-', linewidth=2, label='Sigmoid')
plt.plot(x_demo, y_relu / 10, 'g-', linewidth=2, label='ReLU (scalat /10)')
plt.grid(True, alpha=0.3)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Sigmoid vs ReLU')
plt.legend()
plt.xlim(-10, 10)

# Subplot 3: Derivata sigmoid
plt.subplot(1, 3, 3)
y_derivative = y_sigmoid * (1 - y_sigmoid)
plt.plot(x_demo, y_derivative, 'r-', linewidth=2)
plt.grid(True, alpha=0.3)
plt.xlabel('x')
plt.ylabel("Ïƒ'(x)")
plt.title('Derivata Sigmoid\nÏƒ\'(x) = Ïƒ(x)(1-Ïƒ(x))')
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

# GenereazÄƒ 100 de valori aleatorii Ã®ntre -10 È™i 10
print("\nAPLICARE PE DATASET ALEATORIU:")
print("-" * 40)

date_aleatorii = np.random.uniform(-10, 10, 100)
print(f"Am generat 100 de valori aleatorii Ã®n intervalul [-10, 10]")

# AplicÄƒ sigmoid pe toate valorile (vectorizat - fÄƒrÄƒ loop!)
rezultate = sigmoid(date_aleatorii)

# Vizualizare distribuÈ›ii
plt.figure(figsize=(14, 6))

# Subplot 1: DistribuÈ›ia valorilor iniÈ›iale
plt.subplot(2, 3, 1)
plt.hist(date_aleatorii, bins=20, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)
plt.title('DistribuÈ›ia valorilor iniÈ›iale')
plt.xlabel('Valoare')
plt.ylabel('FrecvenÈ›Äƒ')

# Subplot 2: DistribuÈ›ia dupÄƒ sigmoid
plt.subplot(2, 3, 2)
plt.hist(rezultate, bins=20, alpha=0.7, color='green', edgecolor='black')
plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5)
plt.title('DistribuÈ›ia dupÄƒ aplicarea sigmoid')
plt.xlabel('Valoare')
plt.ylabel('FrecvenÈ›Äƒ')
plt.xlim(0, 1)

# Subplot 3: Scatter plot pentru a vedea transformarea
plt.subplot(2, 3, 3)
plt.scatter(date_aleatorii, rezultate, alpha=0.5)
plt.plot(x_demo, sigmoid(x_demo), 'r-', linewidth=2, alpha=0.5)
plt.xlabel('Valori iniÈ›iale')
plt.ylabel('DupÄƒ sigmoid')
plt.title('Transformarea sigmoid')
plt.grid(True, alpha=0.3)

# Subplot 4: Box plots comparative
plt.subplot(2, 3, 4)
plt.boxplot([date_aleatorii, rezultate], labels=['IniÈ›ial', 'DupÄƒ sigmoid'])
plt.title('ComparaÈ›ie Box Plot')
plt.ylabel('Valoare')
plt.grid(True, alpha=0.3)

# Subplot 5: CDF (Cumulative Distribution Function)
plt.subplot(2, 3, 5)
sorted_initial = np.sort(date_aleatorii)
sorted_sigmoid = np.sort(rezultate)
plt.plot(sorted_initial, np.arange(len(sorted_initial)) / len(sorted_initial),
         'b-', label='IniÈ›ial', linewidth=2)
plt.plot(sorted_sigmoid, np.arange(len(sorted_sigmoid)) / len(sorted_sigmoid),
         'g-', label='DupÄƒ sigmoid', linewidth=2)
plt.xlabel('Valoare')
plt.ylabel('CDF')
plt.title('FuncÈ›ii de DistribuÈ›ie Cumulative')
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 6: Q-Q plot
plt.subplot(2, 3, 6)
plt.scatter(np.sort(date_aleatorii), np.sort(rezultate), alpha=0.5)
plt.xlabel('Quantile iniÈ›iale')
plt.ylabel('Quantile dupÄƒ sigmoid')
plt.title('Q-Q Plot')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Statistici detaliate
print("\nSTATISTICI COMPARATIVE:")
print("-" * 40)
print(f"{'MetricÄƒ':<20} {'IniÈ›ial':>15} {'DupÄƒ Sigmoid':>15}")
print("-" * 50)
print(f"{'Media':<20} {np.mean(date_aleatorii):>15.3f} {np.mean(rezultate):>15.3f}")
print(f"{'Mediana':<20} {np.median(date_aleatorii):>15.3f} {np.median(rezultate):>15.3f}")
print(f"{'DeviaÈ›ia standard':<20} {np.std(date_aleatorii):>15.3f} {np.std(rezultate):>15.3f}")
print(f"{'Minim':<20} {np.min(date_aleatorii):>15.3f} {np.min(rezultate):>15.3f}")
print(f"{'Maxim':<20} {np.max(date_aleatorii):>15.3f} {np.max(rezultate):>15.3f}")
print(f"{'Percentila 25':<20} {np.percentile(date_aleatorii, 25):>15.3f} {np.percentile(rezultate, 25):>15.3f}")
print(f"{'Percentila 75':<20} {np.percentile(date_aleatorii, 75):>15.3f} {np.percentile(rezultate, 75):>15.3f}")

# ObservaÈ›ii despre transformare
print("\nOBSERVAÈšII CHEIE:")
print("-" * 40)
print("1. Sigmoid comprimÄƒ TOATE valorile Ã®n intervalul (0, 1)")
print("2. Valorile extreme (Â±10) devin foarte aproape de 0 sau 1")
print("3. Valorile din jurul lui 0 sunt mapate Ã®n jurul lui 0.5")
print("4. DistribuÈ›ia devine mai 'concentratÄƒ' Ã®n mijloc")
print("5. RelaÈ›iile de ordine sunt pÄƒstrate (monotonie)")

# =============================================================================
# EXERCIÈšIUL 3: ReÈ›ea NeuronalÄƒ cu 2 Straturi (Fully Connected)
# =============================================================================

print("\n" + "=" * 80)
print("EXERCIÈšIUL 3: ReÈ›ea NeuronalÄƒ cu 2 Straturi")
print("=" * 80)

"""
Research Notes - ReÈ›ele Neuronale Multi-Layer
----------------------------------------------
De ce avem nevoie de mai multe straturi?
- Un singur neuron = doar decizii liniare
- Mai multe straturi = pot Ã®nvÄƒÈ›a funcÈ›ii complexe, non-liniare
- Teorema aproximÄƒrii universale: o reÈ›ea cu 1 strat ascuns poate aproxima orice funcÈ›ie

Fully Connected (Dense) Layers:
- Fiecare neuron din stratul N e conectat la TOÈšI neuronii din stratul N+1
- NumÄƒr parametri Ã®ntre straturi: (neuroni_strat_N Ã— neuroni_strat_N+1) + neuroni_strat_N+1

Forward Propagation:
1. Input â†’ Linear transformation (W1Â·x + b1) â†’ Activare â†’ Hidden Layer
2. Hidden â†’ Linear transformation (W2Â·h + b2) â†’ Activare â†’ Output

IniÈ›ializarea parametrilor:
- Prea mici â†’ semnalul dispare (vanishing)
- Prea mari â†’ semnalul explodeazÄƒ (exploding)
- SoluÈ›ii: Xavier/He initialization

Broadcasting Ã®n NumPy:
- Permite operaÈ›ii eficiente pe batch-uri
- Ex: (batch_size, features) Ã— (features, neurons) = (batch_size, neurons)
"""


class ReteasNeuronala:
    def __init__(self, dim_intrare, dim_ascuns, dim_iesire):
        """
        IniÈ›ializeazÄƒ o reÈ›ea neuronalÄƒ cu 2 straturi dense

        Arhitectura:
        Input (dim_intrare) â†’ Hidden (dim_ascuns) â†’ Output (dim_iesire)

        Parameters:
        -----------
        dim_intrare : int
            NumÄƒrul de features de intrare
        dim_ascuns : int
            NumÄƒrul de neuroni Ã®n stratul ascuns
        dim_iesire : int
            NumÄƒrul de neuroni de ieÈ™ire (clase)
        """
        print(f"IniÈ›ializare reÈ›ea neuronalÄƒ:")
        print(f"  ArhitecturÄƒ: {dim_intrare} â†’ {dim_ascuns} â†’ {dim_iesire}")

        # STRATUL 1: Input â†’ Hidden
        # W1 shape: (dim_intrare, dim_ascuns)
        # IniÈ›ializare Xavier/Glorot pentru ReLU
        self.W1 = np.random.randn(dim_intrare, dim_ascuns) * np.sqrt(2.0 / dim_intrare)
        self.b1 = np.zeros((1, dim_ascuns))  # Shape (1, dim_ascuns) pentru broadcasting

        # STRATUL 2: Hidden â†’ Output
        # W2 shape: (dim_ascuns, dim_iesire)
        self.W2 = np.random.randn(dim_ascuns, dim_iesire) * np.sqrt(2.0 / dim_ascuns)
        self.b2 = np.zeros((1, dim_iesire))

        # CalculÄƒm numÄƒrul total de parametri
        total_params = (dim_intrare * dim_ascuns + dim_ascuns +  # Stratul 1
                        dim_ascuns * dim_iesire + dim_iesire)  # Stratul 2

        print(f"  Total parametri antrenabili: {total_params}")
        print(f"  Dimensiuni matrici:")
        print(f"    W1: {self.W1.shape}, b1: {self.b1.shape}")
        print(f"    W2: {self.W2.shape}, b2: {self.b2.shape}")

    def relu(self, Z):
        """
        ImplementeazÄƒ ReLU vectorizat
        FuncÈ›ioneazÄƒ pe matrici de orice dimensiune
        """
        return np.maximum(0, Z)

    def sigmoid(self, Z):
        """
        ImplementeazÄƒ sigmoid vectorizat
        Include clipping pentru stabilitate numericÄƒ
        """
        Z_safe = np.clip(Z, -500, 500)
        return 1 / (1 + np.exp(-Z_safe))

    def forward(self, X):
        """
        Forward pass prin reÈ›ea - COMPLET VECTORIZAT

        Parameters:
        -----------
        X : numpy.ndarray
            Matrice de intrare cu shape (batch_size, dim_intrare)
            Fiecare rÃ¢nd = un exemplu

        Returns:
        --------
        numpy.ndarray
            ProbabilitÄƒÈ›i de ieÈ™ire cu shape (batch_size, dim_iesire)

        Proces:
        -------
        1. Z1 = X @ W1 + b1        (transformare liniarÄƒ)
        2. A1 = ReLU(Z1)          (activare non-liniarÄƒ)
        3. Z2 = A1 @ W2 + b2      (transformare liniarÄƒ)
        4. A2 = Sigmoid(Z2)       (activare â†’ probabilitÄƒÈ›i)
        """
        # STRATUL 1: Input â†’ Hidden cu ReLU
        # Matrix multiplication: (batch_size, dim_intrare) @ (dim_intrare, dim_ascuns)
        # Result: (batch_size, dim_ascuns)
        Z1 = np.dot(X, self.W1) + self.b1  # Broadcasting adaugÄƒ bias la fiecare exemplu
        A1 = self.relu(Z1)  # Activare element-wise

        # SalvÄƒm pentru debugging/vizualizare
        self.Z1 = Z1
        self.A1 = A1

        # STRATUL 2: Hidden â†’ Output cu Sigmoid
        # Matrix multiplication: (batch_size, dim_ascuns) @ (dim_ascuns, dim_iesire)
        # Result: (batch_size, dim_iesire)
        Z2 = np.dot(A1, self.W2) + self.b2
        A2 = self.sigmoid(Z2)

        # SalvÄƒm pentru debugging
        self.Z2 = Z2
        self.A2 = A2

        return A2

    def prezice(self, X):
        """
        ReturneazÄƒ predicÈ›ii binare (0 sau 1)
        FoloseÈ™te pragul standard de 0.5 pentru clasificare binarÄƒ
        """
        output = self.forward(X)
        return (output > 0.5).astype(int)

    def vizualizeaza_activari(self, X):
        """
        FuncÈ›ie helper pentru a vizualiza activÄƒrile prin reÈ›ea
        """
        _ = self.forward(X)  # RulÄƒm forward pass

        print("\nVIZUALIZARE ACTIVÄ‚RI:")
        print("-" * 40)
        print(f"Input shape: {X.shape}")
        print(f"DupÄƒ stratul 1 (Ã®nainte de ReLU): {self.Z1.shape}")
        print(f"DupÄƒ ReLU: {self.A1.shape}")
        print(f"DupÄƒ stratul 2 (Ã®nainte de sigmoid): {self.Z2.shape}")
        print(f"Output final: {self.A2.shape}")

        # Statistici despre activÄƒri
        print(f"\nStatistici activÄƒri:")
        print(f"Neuroni activi Ã®n hidden layer: {np.mean(self.A1 > 0) * 100:.1f}%")
        print(f"Sparsitate hidden layer: {np.mean(self.A1 == 0) * 100:.1f}%")

        return self.A1, self.A2


# Testare ReÈ›ea NeuronalÄƒ
print("\nTESTARE REÈšEA NEURONALÄ‚:")
print("-" * 40)

# CreeazÄƒ o reÈ›ea: 4 intrÄƒri â†’ 5 neuroni ascunÈ™i â†’ 2 ieÈ™iri
retea = ReteasNeuronala(dim_intrare=4, dim_ascuns=5, dim_iesire=2)

# Date de test: 10 exemple cu 4 features fiecare
# SimulÄƒm un mini-batch de date
X_test = np.random.randn(10, 4)
print(f"\nDate de test generate: {X_test.shape[0]} exemple, {X_test.shape[1]} features")

# ObÈ›ine predicÈ›iile
print("\nRulare forward pass...")
predictii = retea.forward(X_test)
clase_prezise = retea.prezice(X_test)

print("\n" + "=" * 60)
print("REZULTATE:")
print("=" * 60)
print(f"Forma intrÄƒrii: {X_test.shape}")
print(f"Forma ieÈ™irii: {predictii.shape}")

print("\nPrimele 5 exemple:")
print("-" * 40)
for i in range(5):
    print(f"Exemplu {i + 1}:")
    print(f"  Input: {X_test[i]}")
    print(f"  ProbabilitÄƒÈ›i: {predictii[i]}")
    print(f"  ClasÄƒ prezisÄƒ: {clase_prezise[i]}")

# VerificÄƒ vectorizarea
print("\n" + "=" * 60)
print("VERIFICARE VECTORIZARE:")
print("=" * 60)
print(f"Dimensiuni parametri:")
print(f"  W1: {retea.W1.shape} = {retea.W1.shape[0]}Ã—{retea.W1.shape[1]} = {np.prod(retea.W1.shape)} parametri")
print(f"  b1: {retea.b1.shape} = {np.prod(retea.b1.shape)} parametri")
print(f"  W2: {retea.W2.shape} = {retea.W2.shape[0]}Ã—{retea.W2.shape[1]} = {np.prod(retea.W2.shape)} parametri")
print(f"  b2: {retea.b2.shape} = {np.prod(retea.b2.shape)} parametri")
print(
    f"  TOTAL: {np.prod(retea.W1.shape) + np.prod(retea.b1.shape) + np.prod(retea.W2.shape) + np.prod(retea.b2.shape)} parametri")

# Vizualizare activÄƒri
activari_hidden, activari_output = retea.vizualizeaza_activari(X_test)

# Test de performanÈ›Äƒ - verificÄƒm cÄƒ e Ã®ntr-adevÄƒr vectorizat
print("\n" + "=" * 60)
print("TEST PERFORMANÈšÄ‚ VECTORIZARE:")
print("=" * 60)

import time

# Test cu batch mic
X_small = np.random.randn(100, 4)
start = time.time()
_ = retea.forward(X_small)
time_small = time.time() - start

# Test cu batch mare
X_large = np.random.randn(10000, 4)
start = time.time()
_ = retea.forward(X_large)
time_large = time.time() - start

print(f"Timp pentru 100 exemple: {time_small * 1000:.2f} ms")
print(f"Timp pentru 10,000 exemple: {time_large * 1000:.2f} ms")
print(f"Speedup: {(time_small * 100) / (time_large):.1f}x (ideal ar fi ~1x)")
print("(DacÄƒ speedup-ul e aproape de 1, vectorizarea funcÈ›ioneazÄƒ corect!)")

# Vizualizare distribuÈ›ia output-urilor
plt.figure(figsize=(15, 5))

# Subplot 1: Histograma probabilitÄƒÈ›ilor pentru clasa 1
plt.subplot(1, 3, 1)
plt.hist(predictii[:, 0], bins=20, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Prag decizie')
plt.xlabel('Probabilitate')
plt.ylabel('FrecvenÈ›Äƒ')
plt.title('DistribuÈ›ia probabilitÄƒÈ›ilor\npentru Clasa 0')
plt.legend()

# Subplot 2: Histograma probabilitÄƒÈ›ilor pentru clasa 2
plt.subplot(1, 3, 2)
plt.hist(predictii[:, 1], bins=20, alpha=0.7, color='green', edgecolor='black')
plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Prag decizie')
plt.xlabel('Probabilitate')
plt.ylabel('FrecvenÈ›Äƒ')
plt.title('DistribuÈ›ia probabilitÄƒÈ›ilor\npentru Clasa 1')
plt.legend()

# Subplot 3: Scatter plot probabilitÄƒÈ›i clasa 0 vs clasa 1
plt.subplot(1, 3, 3)
colors = ['red' if c[0] == 1 else 'blue' for c in clase_prezise]
plt.scatter(predictii[:, 0], predictii[:, 1], c=colors, alpha=0.6, s=50)
plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)
plt.xlabel('Probabilitate Clasa 0')
plt.ylabel('Probabilitate Clasa 1')
plt.title('SpaÈ›iul probabilitÄƒÈ›ilor\n(roÈ™u=prezis clasa 0, albastru=prezis clasa 1)')
plt.xlim(0, 1)
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

# =============================================================================
# SUMAR È˜I CONCLUZII
# =============================================================================

print("\n" + "=" * 80)
print("SUMAR È˜I CONCLUZII")
print("=" * 80)

print("""
Ce am Ã®nvÄƒÈ›at Ã®n aceastÄƒ temÄƒ:
-------------------------------

1. NEURON SIMPLU (ExerciÈ›iul 1):
   âœ“ Un neuron calculeazÄƒ o sumÄƒ ponderatÄƒ È™i aplicÄƒ o funcÈ›ie de activare
   âœ“ ReLU introduce non-linearitate pÄƒstrÃ¢nd eficienÈ›a computaÈ›ionalÄƒ
   âœ“ Weights È™i bias determinÄƒ comportamentul neuronului

2. FUNCÈšIA SIGMOID (ExerciÈ›iul 2):
   âœ“ Sigmoid mapeazÄƒ valori Ã®n (0,1) - perfect pentru probabilitÄƒÈ›i
   âœ“ Are derivatÄƒ convenabilÄƒ dar suferÄƒ de vanishing gradient
   âœ“ NumPy vectorizeazÄƒ automat operaÈ›iile pe arrays

3. REÈšEA MULTI-LAYER (ExerciÈ›iul 3):
   âœ“ Mai multe straturi = capacitate de a Ã®nvÄƒÈ›a funcÈ›ii complexe
   âœ“ Vectorizarea permite procesare eficientÄƒ de batch-uri
   âœ“ Broadcasting-ul NumPy eliminÄƒ necesitatea loop-urilor

Concepte NumPy esenÈ›iale demonstrate:
-------------------------------------
â€¢ np.dot() - multiplicare matricealÄƒ pentru propagare forward
â€¢ np.maximum() - operaÈ›ii element-wise pentru funcÈ›ii de activare  
â€¢ Broadcasting - adÄƒugare eficientÄƒ de bias la batch-uri Ã®ntregi
â€¢ Vectorizare - procesare simultanÄƒ a mai multor exemple

ÃntrebÄƒri pentru explorare viitoare:
------------------------------------
? Cum se antreneazÄƒ aceÈ™ti parametri? (Backpropagation)
? Cum alegem arhitectura optimÄƒ? (Hyperparameter tuning)
? Ce alte funcÈ›ii de activare existÄƒ? (Leaky ReLU, Swish, GELU)
? Cum prevenim overfitting? (Dropout, regularizare)
? Cum iniÈ›ializÄƒm parametrii optim? (Xavier, He, LSUV)

AceastÄƒ implementare reprezintÄƒ FUNDAMENTUL pentru:
- Clasificare (binary/multiclass)
- Regresie
- Deep Learning frameworks (PyTorch, TensorFlow)
- ÃnÈ›elegerea reÈ›elelor moderne (CNNs, Transformers)
""")

print("\n" + "=" * 80)
print("TEMÄ‚ COMPLETATÄ‚ CU SUCCES! ğŸ‰")
print("=" * 80)