import numpy as np

# ============================================================
# ğŸ¤– ReÈ›ele Neuronale cu NumPy â€” 4D Thinking
# ============================================================
# NumPy este baza matematicÄƒ pentru toate bibliotecile AI moderne:
#   â€¢ TensorFlow
#   â€¢ PyTorch
#   â€¢ scikit-learn
#
# Toate acestea se bazeazÄƒ pe operaÈ›ii fundamentale de algebra liniarÄƒ:
#   - Ã®nmulÈ›iri de matrici (dot product)
#   - funcÈ›ii de activare (tanh, relu, sigmoid)
#   - transformÄƒri multi-dimensionale (tensori)
#
# ------------------------------------------------------------
# ğŸ§  Model simplificat de reÈ›ea neuronalÄƒ:
#   Input Layer: 784 neuroni  (imagine 28Ã—28 pixeli)
#   Hidden Layer: 128 neuroni (caracteristici intermediare)
#   Output Layer: 10 neuroni  (probabilitÄƒÈ›i pentru cifrele 0â€“9)
# ============================================================


# ------------------------------------------------------------
# 1ï¸âƒ£ Definim o reÈ›ea neuronalÄƒ simplÄƒ
# ------------------------------------------------------------
class SimpleNeuralNetwork:
    def __init__(self):
        # Matricile de greutÄƒÈ›i (Weights)
        # np.random.randn() â†’ distribuÈ›ie normalÄƒ (mean=0, std=1)
        # *0.1 â†’ scalare micÄƒ pentru stabilitate numericÄƒ
        self.W1 = np.random.randn(784, 128) * 0.1  # input â†’ hidden
        self.W2 = np.random.randn(128, 10) * 0.1   # hidden â†’ output

    def forward(self, X):
        # --------------------------------------------------------
        # Propagarea Ã®nainte (Forward Pass)
        # --------------------------------------------------------
        # 1. Intrarea (X) are forma (batch_size, 784)
        #    â†’ fiecare rÃ¢nd = o imagine aplatizatÄƒ (28Ã—28)
        #
        # 2. np.dot(X, W1) â†’ Ã®nmulÈ›ire matricealÄƒ
        #    (batch_size Ã— 784) Â· (784 Ã— 128) = (batch_size Ã— 128)
        self.z1 = np.dot(X, self.W1)

        # 3. FuncÈ›ie de activare tanh (non-liniaritate)
        #    Se aplicÄƒ element cu element
        self.a1 = np.tanh(self.z1)

        # 4. Al doilea strat (hidden â†’ output)
        #    (batch_size Ã— 128) Â· (128 Ã— 10) = (batch_size Ã— 10)
        self.z2 = np.dot(self.a1, self.W2)

        # 5. np.softmax() â†’ transformÄƒ scorurile brute Ã®n probabilitÄƒÈ›i
        return self.softmax(self.z2)

    def softmax(self, z):
        # --------------------------------------------------------
        # Softmax = e^(zi) / Î£(e^(zj))
        # NormalizeazÄƒ fiecare rÃ¢nd la sumÄƒ = 1 (probabilitÄƒÈ›i)
        # --------------------------------------------------------
        exp_values = np.exp(z - np.max(z, axis=1, keepdims=True))  # stabil numeric
        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
        return probabilities


# ------------------------------------------------------------
# 2ï¸âƒ£ TestÄƒm reÈ›eaua cu un "batch" de imagini
# ------------------------------------------------------------
# SÄƒ presupunem cÄƒ avem 5 imagini de intrare (batch_size = 5)
# Fiecare imagine = vector de 784 valori normalizate (0â€“1)
X = np.random.rand(5, 784)

# CreÄƒm reÈ›eaua È™i rulÄƒm propagarea
nn = SimpleNeuralNetwork()
output = nn.forward(X)

print("ğŸ”¢ Forma ieÈ™irii (batch_size Ã— num_clase):", output.shape)
print("ğŸ“Š Primele probabilitÄƒÈ›i:\n", output[0])

# Exemplu de rezultat:
# ğŸ”¢ Forma ieÈ™irii (batch_size Ã— num_clase): (5, 10)
# ğŸ“Š Primele probabilitÄƒÈ›i:
# [0.105 0.096 0.098 0.097 0.101 0.098 0.097 0.103 0.099 0.106]
